import numpy as np
import matplotlib.pyplot as plt
import math
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import copy

#Initialise data

df = pd.read_csv(r'C:\Users\joshi\OneDrive\Desktop\ML Projects\Life Expectancy Data.csv')

y_data = df['Life expectancy ']
x_data = df.drop(['Life expectancy ', 'Country', 'Status'], axis=1)

# Fill missing values
# For numeric columns, use median; for categorical, use the most frequent value
numeric_features = x_data.select_dtypes(include=['int64', 'float64']).columns
categorical_features = x_data.select_dtypes(include=['object']).columns

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])



x_data = preprocessor.fit_transform(x_data)

# Pipeline for y_data
y_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  
    ('scaler', StandardScaler())
])

# Fit and transform y_data
y_data = y_transformer.fit_transform(y_data.values.reshape(-1, 1))  # Reshape for a single column
m = len(y_data)


#normalizing data
mu = np.mean(x_data,axis=0)
sigma = np.std(x_data,axis=0)

mu_y = np.mean(y_data)
sigma_y = np.std(y_data)

def fnormalize(X,mu,sigma):
    X_norm = np.zeros(X.shape)
    for i in range(X.shape[1]):
        temp_X = X[:,i]
        temp_X = (temp_X-mu[i])/sigma[i]
        X_norm[:,i]=temp_X
    return X_norm


X_norm = fnormalize(x_data,mu,sigma)
Y_norm = (y_data - mu_y)/sigma_y


num_rows = X_norm.shape[1]
w = np.ones(num_rows)
b = 0.02

def compute_cost(X, y, w, b, lambda_=1): 
    """
    compute cost
    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters  
      b (scalar)       : model parameter
      
    Returns:
      cost (scalar): cost
    """
    m = X.shape[0]
    n  = len(w)
    cost = 0
    for i in range(m):                                
        f_wb_i = np.dot(X[i], w) + b
        cost = cost + (f_wb_i - y[i])**2
    cost = cost / (2 * m)

    reg_cost = 0
    for j in range(n):
        reg_cost += (w[j]**2)                                         
    reg_cost = (lambda_/(2*m)) * reg_cost 

    #regularization
    cost = cost + reg_cost
    cost = np.ndarray.item(cost) 
    return cost


def compute_gradient(X, y, w, b, lambda_=1): 
    """
    Computes the gradient for linear regression 
    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters  
      b (scalar)       : model parameter
      
    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape           
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):                             
        err = (np.dot(X[i], w) + b) - y[i]   
        for j in range(n):                         
            dj_dw[j] = dj_dw[j] + err * X[i, j]    
        dj_db = dj_db + err                        
    dj_dw = dj_dw / m                                
    dj_db = dj_db / m

    for j in range(n):
        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]                                
        
    return dj_db, dj_dw


def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): 
    """
    Performs batch gradient descent to learn theta. Updates theta by taking 
    num_iters gradient steps with learning rate alpha
    
    Args:
      X (ndarray (m,n))   : Data, m examples with n features
      y (ndarray (m,))    : target values
      w_in (ndarray (n,)) : initial model parameters  
      b_in (scalar)       : initial model parameter
      cost_function       : function to compute cost
      gradient_function   : function to compute the gradient
      alpha (float)       : Learning rate
      num_iters (int)     : number of iterations to run gradient descent
      
    Returns:
      w (ndarray (n,)) : Updated values of parameters 
      b (scalar)       : Updated value of parameter 
      """
    
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    w = copy.deepcopy(w_in)  #avoid modifying global w within function
    b = b_in
    
    for i in range(num_iters):

        # Calculate the gradient and update the parameters
        dj_db,dj_dw = gradient_function(X, y, w, b)  

        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw              
        b = b - alpha * dj_db               
      
        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion 
            J_history.append( cost_function(X, y, w, b))

        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters / 100) == 0:
            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   ")
            print(w)
            print(b)
        
    return w, b, J_history #return final w,b and J history for graphing


# gGradient descent settings
iterations = 100
alpha = 5.0e-2
# run gradient descent 
w_final, b_final, J_hist = gradient_descent(X_norm, Y_norm, w, b, compute_cost, compute_gradient, alpha, iterations)


X_features = ['Year','Adult Mortality','infant deaths','Alcohol','percentage expenditure','Hepatitis B','Measles ', 'BMI' ,'under-five deaths' ,'Polio','Total expenditure','Diphtheria ', 'HIV/AIDS','GDP','Population', 'thinness  1-19 years', 'thinness 5-9 years','Income composition of resources','Schooling']

fig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True)
for i in range(len(ax)):
    ax[i].scatter(x_data[:,i],y_data, label = 'target')
    ax[i].set_xlabel(X_features[i])
    ax[i].scatter(x_data[:,i],(np.dot(X_norm, w_final) + b_final), label = 'predict')
ax[0].set_ylabel("Life Expectancy"); ax[0].legend();
fig.suptitle("target versus prediction using z-score normalized model")
plt.show()
