import numpy as np
import matplotlib.pyplot as plt
import math
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import Sequential
from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy
from tensorflow.keras.activations import sigmoid
#from lab_neurons_utils import plt_prob_1d, sigmoidnp, plt_linear, plt_logistic
import copy


df = pd.read_csv(r'C:\Users\joshi\OneDrive\Desktop\ML Projects\Life Expectancy Data.csv')

y_data = df['Life expectancy ']
x_data = df.drop(['Life expectancy ', 'Country', 'Status'], axis=1)

# Fill missing values
# For numeric columns, use median; for categorical, use the most frequent value
numeric_features = x_data.select_dtypes(include=['int64', 'float64']).columns
categorical_features = x_data.select_dtypes(include=['object']).columns

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])



x_data = preprocessor.fit_transform(x_data)

# Pipeline for y_data
y_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')), 
    ('scaler', StandardScaler())
])

# Fit and transform y_data
y_data = y_transformer.fit_transform(y_data.values.reshape(-1, 1))  # Reshape for a single column
m = len(y_data)




#normalizing data
mu = np.mean(x_data,axis=0)
sigma = np.std(x_data,axis=0)

mu_y = np.mean(y_data)
sigma_y = np.std(y_data)

def fnormalize(X,mu,sigma):
    X_norm = np.zeros(X.shape)
    for i in range(X.shape[1]):
        temp_X = X[:,i]
        temp_X = (temp_X-mu[i])/sigma[i]
        X_norm[:,i]=temp_X
    return X_norm



X_norm = fnormalize(x_data,mu,sigma)
Y_norm = (y_data - mu_y)/sigma_y


def my_dense(a_in, W, b):
    """
    Computes dense layer
    Args:
      a_in (ndarray (n, )) : Data, 1 example 
      W    (ndarray (n,j)) : Weight matrix, n features per unit, j units
      b    (ndarray (j, )) : bias vector, j units  
    Returns
      a_out (ndarray (j,))  : j units|
    """
    units = W.shape[1]
    a_out = np.zeros(units)
    for j in range(units):               
        w = W[:,j]                                    
        z = np.dot(w, a_in) + b[j]         
        a_out[j] = g(z)               
    return(a_out)


def my_sequential(x, W1, b1, W2, b2):
    a1 = my_dense(x,  W1, b1)
    a2 = my_dense(a1, W2, b2)
    return(a2)


W1_tmp = np.array( [[]] )
b1_tmp = np.array( [] )
W2_tmp = np.array( [] )
b2_tmp = np.array( [] )


def my_predict(X, W1, b1, W2, b2):
    m = X.shape[0]
    p = np.zeros((m,1))
    for i in range(m):
        p[i,0] = my_sequential(X[i], W1, b1, W2, b2)
    return(p)
